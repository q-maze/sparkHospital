{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 | Model Building | Logistic Regression with Downsampling\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session and load the data from the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04Logistic\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline, PipelineModel  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/project/ds5559/fa21-group04/data/processed_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a training/validation test split on the data, reserving 80% of the data for training and validation and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal, test = df.randomSplit([0.8, 0.2], seed=304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigated if downsampling our data to remove the imbalance in `hospital_death` would improve the performance of our model. To do this, two dataframes are constructed by filtering the data based on `hospital_death` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_df = trainVal.filter(col(\"hospital_death\") == 1)\n",
    "life_df = trainVal.filter(col(\"hospital_death\") == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the downsampling ratio is determined by ratio of `hospital_death = 1` to `hospital_death = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = int(life_df.count() / death_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ratio is approximately 10:1, which indicates that there is moderate imbalance present in our response variable. We then account for this imbalance by sampling `life_df` using the inverse of this ratio and unioning the result to the `death_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_life_df = life_df.sample(False, 1/ratio, seed=304)\n",
    "trainValDownsampled = sampled_life_df.unionAll(death_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a feature vector is constructed using the selected features from the `Group04FeatureSelection.ipynb` notebook. The columns selected as inputs are features chosen by a `UnivariateFeatureSelector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_vectorizer =  VectorAssembler(inputCols=['FinalCatFeatures',\n",
    "                                                       'selectedContFeatures'],\n",
    "                                            outputCol='features',\n",
    "                                            handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we select the features we want in model manually.from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol='hospital_death', featuresCol='features', maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline(stages=[final_feature_vectorizer,\n",
    "                               lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setLabelCol(lr.getLabelCol()),\n",
    "                          numFolds=5,\n",
    "                          seed=304,\n",
    "                          parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg_model = crossval.fit(trainValDownsampled)\n",
    "log_reg_model = CrossValidatorModel.load('/project/ds5559/fa21-group04/models/logregmodeldown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, the test set is predicted using the chosen model from the `CrossValidator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = log_reg_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are then passed to two objects:\n",
    "\n",
    "* `BinaryClassificationEvaluator` - for AUC metric\n",
    "* `MulticlassClassificationEvaluator` - for confusion matrix and all other model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator = BinaryClassificationEvaluator(rawPredictionCol='probability',\n",
    "                                              labelCol='hospital_death',\n",
    "                                              metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator2 = MulticlassClassificationEvaluator(labelCol='hospital_death',\n",
    "                                                   predictionCol=\"prediction\",\n",
    "                                                   probabilityCol='probability',\n",
    "                                                   metricLabel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7941048034934498"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.262782401902497"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"precisionByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7169505271695052"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"recallByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38459865129432236"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"fMeasureByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7169505271695052"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"truePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1982889581834173"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"falsePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8387229834180133"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator.evaluate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10027.,  2480.],\n",
       "       [  349.,   884.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = preds.select(['prediction','hospital_death']).withColumn('label', F.col('hospital_death').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "conf_matrix = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "conf_matrix.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|10376|\n",
      "|       1.0| 3364|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other models, it is clear from the low values of the model metrics and the high AUC, that the imbalanced dataset coupled with Spark's default behavior of selecting the higher porbability result as the reported result, is causing the metrics to suffer. Therefore, a probability cutoff value must be selected to reduce the effects of the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg_model.write().overwrite().save(\"/project/ds5559/fa21-group04/models/logregmodeldown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_468318d65ca1', name='regParam', doc='regularization parameter (>= 0).'): 0.01}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_model.getEstimatorParamMaps()[ np.argmax(log_reg_model.avgMetrics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen value for `regParam` is 0.01, indicating that there is some small benefit in applying a penalty term to the logisitic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Threshold Determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "getprob = udf(lambda v:float(v[1]),FloatType())\n",
    "## Select out the necessary columns\n",
    "output = preds.select(col(\"rawPrediction\"),\n",
    "                              col(\"hospital_death\").cast(DoubleType()),\n",
    "                              getprob(col(\"probability\")).alias(\"probability\"),\n",
    "                              col(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cutoff =  0.05\n",
      "Accuracy: 0.1068 Recall: 0.9935 Precision:  0.0908 F1 score: 0.1664 TP 1225 FP 12264 FN 8 TN 243\n",
      "Testing cutoff =  0.10\n",
      "Accuracy: 0.1630 Recall: 0.9903 Precision:  0.0961 F1 score: 0.1752 TP 1221 FP 11488 FN 12 TN 1019\n",
      "Testing cutoff =  0.15\n",
      "Accuracy: 0.2751 Recall: 0.9781 Precision:  0.1083 F1 score: 0.1950 TP 1206 FP 9933 FN 27 TN 2574\n",
      "Testing cutoff =  0.20\n",
      "Accuracy: 0.3951 Recall: 0.9570 Precision:  0.1250 F1 score: 0.2212 TP 1180 FP 8258 FN 53 TN 4249\n",
      "Testing cutoff =  0.25\n",
      "Accuracy: 0.5031 Recall: 0.9286 Precision:  0.1452 F1 score: 0.2512 TP 1145 FP 6739 FN 88 TN 5768\n",
      "Testing cutoff =  0.30\n",
      "Accuracy: 0.5857 Recall: 0.8929 Precision:  0.1653 F1 score: 0.2789 TP 1101 FP 5561 FN 132 TN 6946\n",
      "Testing cutoff =  0.35\n",
      "Accuracy: 0.6531 Recall: 0.8500 Precision:  0.1862 F1 score: 0.3055 TP 1048 FP 4581 FN 185 TN 7926\n",
      "Testing cutoff =  0.40\n",
      "Accuracy: 0.7056 Recall: 0.8102 Precision:  0.2077 F1 score: 0.3306 TP 999 FP 3811 FN 234 TN 8696\n",
      "Testing cutoff =  0.45\n",
      "Accuracy: 0.7531 Recall: 0.7762 Precision:  0.2349 F1 score: 0.3607 TP 957 FP 3117 FN 276 TN 9390\n",
      "Testing cutoff =  0.50\n",
      "Accuracy: 0.7941 Recall: 0.7170 Precision:  0.2628 F1 score: 0.3846 TP 884 FP 2480 FN 349 TN 10027\n",
      "Testing cutoff =  0.55\n",
      "Accuracy: 0.8277 Recall: 0.6659 Precision:  0.2956 F1 score: 0.4095 TP 821 FP 1956 FN 412 TN 10551\n",
      "Testing cutoff =  0.60\n",
      "Accuracy: 0.8535 Recall: 0.6115 Precision:  0.3295 F1 score: 0.4283 TP 754 FP 1534 FN 479 TN 10973\n",
      "Testing cutoff =  0.65\n",
      "Accuracy: 0.8738 Recall: 0.5572 Precision:  0.3664 F1 score: 0.4421 TP 687 FP 1188 FN 546 TN 11319\n",
      "Testing cutoff =  0.70\n",
      "Accuracy: 0.8916 Recall: 0.5061 Precision:  0.4146 F1 score: 0.4558 TP 624 FP 881 FN 609 TN 11626\n",
      "Testing cutoff =  0.75\n",
      "Accuracy: 0.9023 Recall: 0.4290 Precision:  0.4529 F1 score: 0.4406 TP 529 FP 639 FN 704 TN 11868\n",
      "Testing cutoff =  0.80\n",
      "Accuracy: 0.9109 Recall: 0.3569 Precision:  0.5052 F1 score: 0.4183 TP 440 FP 431 FN 793 TN 12076\n",
      "Testing cutoff =  0.85\n",
      "Accuracy: 0.9159 Recall: 0.2814 Precision:  0.5624 F1 score: 0.3751 TP 347 FP 270 FN 886 TN 12237\n",
      "Testing cutoff =  0.90\n",
      "Accuracy: 0.9187 Recall: 0.1987 Precision:  0.6551 F1 score: 0.3049 TP 245 FP 129 FN 988 TN 12378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cutoff: string, accuracy: string, recall: string, precision: string, F1: string, TP: bigint, FP: bigint, FN: bigint, TN: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "performance_df = spark.createDataFrame([(0,0,0,0,0,0,0,0,0)], ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "for cutoff in range(5, 95, 5):\n",
    "    cutoff = (cutoff * 0.01)\n",
    "  \n",
    "    print('Testing cutoff = ', str(format(cutoff, '.2f')))\n",
    "    lrpredictions_prob_temp = output.withColumn('prediction', when(col('probability') >= cutoff, 1).otherwise(0).cast(DoubleType()))\n",
    "    tp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    tn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    fp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    fn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    a = ((tp + tn)/lrpredictions_prob_temp.count())\n",
    "    if(tp + fn == 0.0):\n",
    "        r = 0.0\n",
    "        p = float(tp) / (tp + fp)\n",
    "    elif(tp + fp == 0.0):\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = 0.0\n",
    "    else:\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = float(tp) / (tp + fp)\n",
    "    \n",
    "    if(p + r == 0):\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * ((p * r)/(p + r))\n",
    "    print(\"Accuracy:\", format(a, '.4f'), \"Recall:\", format(r, '.4f'), \"Precision: \", format(p, '.4f'), \"F1 score:\", format(f1, '.4f'), \"TP\", tp, \"FP\", fp, \"FN\", fn, \"TN\", tn)\n",
    "    performance_df_row = spark.createDataFrame([(format(cutoff, '.2f'),format(a, '.4f'), format(r, '.4f'), format(p, '.4f'), format(f1, '.4f'), tp, fp, fn, tn)], \n",
    "                                               ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "    performance_df = performance_df.union(performance_df_row)\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---------+------+----+-----+---+-----+\n",
      "|cutoff|accuracy|recall|precision|    F1|  TP|   FP| FN|   TN|\n",
      "+------+--------+------+---------+------+----+-----+---+-----+\n",
      "|     0|       0|     0|        0|     0|   0|    0|  0|    0|\n",
      "|  0.05|  0.1068|0.9935|   0.0908|0.1664|1225|12264|  8|  243|\n",
      "|  0.10|  0.1630|0.9903|   0.0961|0.1752|1221|11488| 12| 1019|\n",
      "|  0.15|  0.2751|0.9781|   0.1083|0.1950|1206| 9933| 27| 2574|\n",
      "|  0.20|  0.3951|0.9570|   0.1250|0.2212|1180| 8258| 53| 4249|\n",
      "|  0.25|  0.5031|0.9286|   0.1452|0.2512|1145| 6739| 88| 5768|\n",
      "|  0.30|  0.5857|0.8929|   0.1653|0.2789|1101| 5561|132| 6946|\n",
      "|  0.35|  0.6531|0.8500|   0.1862|0.3055|1048| 4581|185| 7926|\n",
      "|  0.40|  0.7056|0.8102|   0.2077|0.3306| 999| 3811|234| 8696|\n",
      "|  0.45|  0.7531|0.7762|   0.2349|0.3607| 957| 3117|276| 9390|\n",
      "|  0.50|  0.7941|0.7170|   0.2628|0.3846| 884| 2480|349|10027|\n",
      "|  0.55|  0.8277|0.6659|   0.2956|0.4095| 821| 1956|412|10551|\n",
      "|  0.60|  0.8535|0.6115|   0.3295|0.4283| 754| 1534|479|10973|\n",
      "|  0.65|  0.8738|0.5572|   0.3664|0.4421| 687| 1188|546|11319|\n",
      "|  0.70|  0.8916|0.5061|   0.4146|0.4558| 624|  881|609|11626|\n",
      "|  0.75|  0.9023|0.4290|   0.4529|0.4406| 529|  639|704|11868|\n",
      "|  0.80|  0.9109|0.3569|   0.5052|0.4183| 440|  431|793|12076|\n",
      "|  0.85|  0.9159|0.2814|   0.5624|0.3751| 347|  270|886|12237|\n",
      "|  0.90|  0.9187|0.1987|   0.6551|0.3049| 245|  129|988|12378|\n",
      "+------+--------+------+---------+------+----+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value for the probability cutoff is 0.7 based on the F1 score. This value is much higher than all the other models due to the fact that this model was trained on downsampled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
