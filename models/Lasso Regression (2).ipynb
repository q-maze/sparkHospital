{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 | Model Building | Lasso Regression\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session and load the data from the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04RF\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline, PipelineModel  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/project/ds5559/fa21-group04/data/processed_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a training/validation test split on the data, reserving 80% of the data for training and validation and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVallasso, testlasso = df.randomSplit([0.8, 0.2], seed=304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a feature vector is constructed using the selected features from the `Group04FeatureSelection.ipynb` notebook. The columns selected as inputs are features chosen by a `UnivariateFeatureSelector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_vectorizer =  VectorAssembler(inputCols=['FinalCatFeatures',\n",
    "                                                       'selectedContFeatures'],\n",
    "                                            outputCol='features',\n",
    "                                            handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a `LassoRegressionClassifier` is constructed to predict `hospital_death` based on the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LogisticRegression(featuresCol = 'features', labelCol = 'hospital_death', elasticNetParam=1.0, maxIter=10, regParam=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline is then created to feed the selected features to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pipeline = Pipeline(stages=[final_feature_vectorizer,\n",
    "                                  lasso])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin model training, the necessary packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our parameter grid for tuning our Ridge Regression model. For this model, our tunable hyperparameter is:\n",
    "\n",
    "* lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lasso.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_lasso = CrossValidator(estimator=lasso_pipeline,\n",
    "                          estimatorParamMaps=lasso_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setLabelCol(lasso.getLabelCol()),\n",
    "                          numFolds=5,\n",
    "                          seed=304,\n",
    "                          parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = crossval_lasso.fit(trainVallasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, the test set is predicted using the chosen model from the `CrossValidator`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predslasso = lasso_model.transform(testlasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are then passed to two objects:\n",
    "\n",
    "* `BinaryClassificationEvaluator` - for AUC metric\n",
    "* `MulticlassClassificationEvaluator` - for confusion matrix and all other model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_evaluator = BinaryClassificationEvaluator(rawPredictionCol='probability',\n",
    "                                              labelCol='hospital_death',\n",
    "                                              metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator_lasso2 = MulticlassClassificationEvaluator(labelCol='hospital_death',\n",
    "                                                   predictionCol=\"prediction\",\n",
    "                                                   probabilityCol='probability',\n",
    "                                                   metricLabel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196245233206218"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_lasso2.evaluate(predslasso, {testEvaluator_lasso2.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6967509025270758"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_lasso2.evaluate(predslasso, {testEvaluator_lasso2.metricName: \"precisionByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16016597510373445"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_lasso2.evaluate(predslasso, {testEvaluator_lasso2.metricName: \"recallByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26045883940620784"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_lasso2.evaluate(predslasso, {testEvaluator_lasso2.metricName: \"fMeasureByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16016597510373445"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_lasso2.evaluate(predslasso, {testEvaluator_lasso2.metricName: \"truePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006757300297642989"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_lasso2.evaluate(predslasso, {testEvaluator_lasso2.metricName: \"falsePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8434704965601"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_evaluator.evaluate(predslasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12347.,    84.],\n",
       "       [ 1012.,   193.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels_lasso = predslasso.select(['prediction','hospital_death']).withColumn('label', F.col('hospital_death').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels_lasso = preds_and_labels_lasso.select(['prediction','label'])\n",
    "conf_matrix_ridge = MulticlassMetrics(preds_and_labels_lasso.rdd.map(tuple))\n",
    "conf_matrix_ridge.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|13359|\n",
      "|       1.0|  277|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predslasso.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_647efb63004c', name='regParam', doc='regularization parameter (>= 0).'): 0.01}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.getEstimatorParamMaps()[ np.argmax(lasso_model.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutoff Value Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the cutoff value is 0.5, however, we wanted to determine a cutoff value that resulted in the highest F1 value. F1 is the weighted average of precision and recall, which we believe is important for this scenario. We decided to not use accuracy as the threshold determination because F1 is usually more useful than accuracy, especially with the uneven class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rawPrediction: vector, hospital_death: double, probability: float, prediction: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "getprob = udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "output = predslasso.select(col(\"rawPrediction\"),\n",
    "                              col(\"hospital_death\").cast(DoubleType()),\n",
    "                              getprob(col(\"probability\")).alias(\"probability\"),\n",
    "                              col(\"prediction\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines each of the different metrics for cutoff values between 0.05 and 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cutoff =  0.05\n",
      "Accuracy: 0.5344 Recall: 0.9245 Precision:  0.1511 F1 score: 0.2598 TP 1114 FP 6258 FN 91 TN 6173\n",
      "Testing cutoff =  0.10\n",
      "Accuracy: 0.8326 Recall: 0.6730 Precision:  0.3004 F1 score: 0.4154 TP 811 FP 1889 FN 394 TN 10542\n",
      "Testing cutoff =  0.15\n",
      "Accuracy: 0.8890 Recall: 0.5386 Precision:  0.4039 F1 score: 0.4616 TP 649 FP 958 FN 556 TN 11473\n",
      "Testing cutoff =  0.20\n",
      "Accuracy: 0.9090 Recall: 0.4407 Precision:  0.4836 F1 score: 0.4611 TP 531 FP 567 FN 674 TN 11864\n",
      "Testing cutoff =  0.25\n",
      "Accuracy: 0.9153 Recall: 0.3660 Precision:  0.5300 F1 score: 0.4330 TP 441 FP 391 FN 764 TN 12040\n",
      "Testing cutoff =  0.30\n",
      "Accuracy: 0.9171 Recall: 0.3054 Precision:  0.5559 F1 score: 0.3942 TP 368 FP 294 FN 837 TN 12137\n",
      "Testing cutoff =  0.35\n",
      "Accuracy: 0.9193 Recall: 0.2664 Precision:  0.5967 F1 score: 0.3683 TP 321 FP 217 FN 884 TN 12214\n",
      "Testing cutoff =  0.40\n",
      "Accuracy: 0.9201 Recall: 0.2299 Precision:  0.6324 F1 score: 0.3372 TP 277 FP 161 FN 928 TN 12270\n",
      "Testing cutoff =  0.45\n",
      "Accuracy: 0.9208 Recall: 0.1925 Precision:  0.6844 F1 score: 0.3005 TP 232 FP 107 FN 973 TN 12324\n",
      "Testing cutoff =  0.50\n",
      "Accuracy: 0.9196 Recall: 0.1602 Precision:  0.6968 F1 score: 0.2605 TP 193 FP 84 FN 1012 TN 12347\n",
      "Testing cutoff =  0.55\n",
      "Accuracy: 0.9190 Recall: 0.1344 Precision:  0.7265 F1 score: 0.2269 TP 162 FP 61 FN 1043 TN 12370\n",
      "Testing cutoff =  0.60\n",
      "Accuracy: 0.9183 Recall: 0.1054 Precision:  0.7791 F1 score: 0.1857 TP 127 FP 36 FN 1078 TN 12395\n",
      "Testing cutoff =  0.65\n",
      "Accuracy: 0.9177 Recall: 0.0830 Precision:  0.8547 F1 score: 0.1513 TP 100 FP 17 FN 1105 TN 12414\n",
      "Testing cutoff =  0.70\n",
      "Accuracy: 0.9160 Recall: 0.0573 Precision:  0.8846 F1 score: 0.1076 TP 69 FP 9 FN 1136 TN 12422\n",
      "Testing cutoff =  0.75\n",
      "Accuracy: 0.9151 Recall: 0.0448 Precision:  0.8852 F1 score: 0.0853 TP 54 FP 7 FN 1151 TN 12424\n",
      "Testing cutoff =  0.80\n",
      "Accuracy: 0.9136 Recall: 0.0266 Precision:  0.8649 F1 score: 0.0515 TP 32 FP 5 FN 1173 TN 12426\n",
      "Testing cutoff =  0.85\n",
      "Accuracy: 0.9125 Recall: 0.0124 Precision:  0.8333 F1 score: 0.0245 TP 15 FP 3 FN 1190 TN 12428\n",
      "Testing cutoff =  0.90\n",
      "Accuracy: 0.9119 Recall: 0.0033 Precision:  0.8000 F1 score: 0.0066 TP 4 FP 1 FN 1201 TN 12430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cutoff: string, accuracy: string, recall: string, precision: string, F1: string, TP: bigint, FP: bigint, FN: bigint, TN: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "performance_df = spark.createDataFrame([(0,0,0,0,0,0,0,0,0)], ['cutoff','accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "for cutoff in range(5, 95, 5):\n",
    "    cutoff = (cutoff * 0.01)\n",
    "  \n",
    "    print('Testing cutoff = ', str(format(cutoff, '.2f')))\n",
    "    lrpredictions_prob_temp = output.withColumn('prediction', when(col('probability') >= cutoff, 1).otherwise(0).cast(DoubleType()))\n",
    "    tp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    tn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    fp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    fn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    a = ((tp + tn)/lrpredictions_prob_temp.count())\n",
    "    if(tp + fn == 0.0):\n",
    "        r = 0.0\n",
    "        p = float(tp) / (tp + fp)\n",
    "    elif(tp + fp == 0.0):\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = 0.0\n",
    "    else:\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = float(tp) / (tp + fp)\n",
    "    \n",
    "    if(p + r == 0):\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * ((p * r)/(p + r))\n",
    "    print(\"Accuracy:\", format(a, '.4f'), \"Recall:\", format(r, '.4f'), \"Precision: \", format(p, '.4f'), \"F1 score:\", format(f1, '.4f'), \"TP\", tp, \"FP\", fp, \"FN\", fn, \"TN\", tn)\n",
    "    performance_df_row = spark.createDataFrame([(format(cutoff, '.2f'), format(a, '.4f'), format(r, '.4f'), format(p, '.4f'), format(f1, '.4f'), tp, fp, fn, tn)], ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "    performance_df = performance_df.union(performance_df_row)\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|cutoff|accuracy|recall|precision|    F1|  TP|  FP|  FN|   TN|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|     0|       0|     0|        0|     0|   0|   0|   0|    0|\n",
      "|  0.05|  0.5344|0.9245|   0.1511|0.2598|1114|6258|  91| 6173|\n",
      "|  0.10|  0.8326|0.6730|   0.3004|0.4154| 811|1889| 394|10542|\n",
      "|  0.15|  0.8890|0.5386|   0.4039|0.4616| 649| 958| 556|11473|\n",
      "|  0.20|  0.9090|0.4407|   0.4836|0.4611| 531| 567| 674|11864|\n",
      "|  0.25|  0.9153|0.3660|   0.5300|0.4330| 441| 391| 764|12040|\n",
      "|  0.30|  0.9171|0.3054|   0.5559|0.3942| 368| 294| 837|12137|\n",
      "|  0.35|  0.9193|0.2664|   0.5967|0.3683| 321| 217| 884|12214|\n",
      "|  0.40|  0.9201|0.2299|   0.6324|0.3372| 277| 161| 928|12270|\n",
      "|  0.45|  0.9208|0.1925|   0.6844|0.3005| 232| 107| 973|12324|\n",
      "|  0.50|  0.9196|0.1602|   0.6968|0.2605| 193|  84|1012|12347|\n",
      "|  0.55|  0.9190|0.1344|   0.7265|0.2269| 162|  61|1043|12370|\n",
      "|  0.60|  0.9183|0.1054|   0.7791|0.1857| 127|  36|1078|12395|\n",
      "|  0.65|  0.9177|0.0830|   0.8547|0.1513| 100|  17|1105|12414|\n",
      "|  0.70|  0.9160|0.0573|   0.8846|0.1076|  69|   9|1136|12422|\n",
      "|  0.75|  0.9151|0.0448|   0.8852|0.0853|  54|   7|1151|12424|\n",
      "|  0.80|  0.9136|0.0266|   0.8649|0.0515|  32|   5|1173|12426|\n",
      "|  0.85|  0.9125|0.0124|   0.8333|0.0245|  15|   3|1190|12428|\n",
      "|  0.90|  0.9119|0.0033|   0.8000|0.0066|   4|   1|1201|12430|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
