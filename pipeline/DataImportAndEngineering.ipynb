{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 | Data Import & Feature Engineering\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04DataImport\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the possible errors and the increased runtime with having Spark infer the schema and to avoid hard-coding the column types we use the supplied data dictionary to create a Spark schema for our data. First, the data dictionary is read into a `pandas` dataframe on the driver node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemadf = pd.read_csv('/project/ds5559/fa21-group04/data/WiDS_Datathon_2020_Dictionary.csv')\n",
    "schemadf = schemadf[schemadf['Variable Name'] != 'icu_admit_type'] # added to address error in data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, spark dataframe column type objects are mapped to the data types as they appear in the data dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# define a python dictionary to map the data types from the data dictionary to the column types in spark schema.\n",
    "data_types = {\n",
    "    'integer': IntegerType(),\n",
    "    'binary': IntegerType(),\n",
    "    'numeric': FloatType(),\n",
    "    'string': StringType()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary comprehension is then used to create a schema for the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pyspark schema for the dataframe\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(row['Variable Name'] , \n",
    "                    data_types[row['Data Type']], \n",
    "                    True) for index, row in schemadf.iterrows()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data is read in using the created schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data using the schema\n",
    "df = spark.read.format('csv') \\\n",
    "    .schema(schema) \\\n",
    "    .option('header', True) \\\n",
    "    .load('/project/ds5559/fa21-group04/data/training_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the data manually, it was identified that `bmi` was type Float, not String as the data dictionary indicated, so this change was manually coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"bmi\",df.bmi.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our group made the decision to remove the columns that contained more than 50% missing data from the dataset. We believe that these columns contained so many missing rows that imputing them could affect the distribution of the data and that any feature engineering based on the missing features could be difficult due to our lack of experience with the subject matter.\n",
    "\n",
    "*Note that the dataframe was selected from its own columns excluding the columns to drop instead of dropping the columns, as this coerced all columns to type string as a byproduct.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "null_counts = df.select([((F.count(F.when(F.isnan(c) | F.col(c).isNull() | F.col(c).contains('NA'), c)))*100/df.count()).alias(c)  for c in df.columns]).collect()[0].asDict()\n",
    "to_drop = [k for k, v in null_counts.items() if v > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, based on the data dictionary, the columns labeled in Category \"APACHE covariate\" are all factors in the variables `apache_4a_icu_death_prob` and `apache_4a_hospital_death_prob` and therefore we can remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apachelist = ['albumin_apache', 'apache_2_diagnosis', 'apache_3j_diagnosis', 'apache_post_operative', 'arf_apache', \n",
    "                'bilirubin_apache', 'bun_apache', 'creatinine_apache', 'fio2_apache', 'gcs_eyes_apache', 'gcs_motor_apache', \n",
    "                'gcs_unable_apache', 'gcs_verbal_apache', 'glucose_apache', 'heart_rate_apache', 'hematocrit_apache', \n",
    "                'intubated_apache', 'map_apache', 'paco2_apache', 'paco2_for_ph_apache', 'pao2_apache', 'ph_apache', \n",
    "                'resprate_apache', 'sodium_apache', 'temp_apache', 'urineoutput_apache', 'ventilated_apache', 'wbc_apache']\n",
    "to_drop = to_drop + apachelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our group also removed the columns labeled in Category indicating \"noninvasive\", as according to the data dictionary they are included in the equivalent measurement which are measured either invasive or non invasive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noninvasivelist = ['d1_diasbp_noninvasive_max', 'd1_diasbp_noninvasive_min', 'd1_mbp_noninvasive_max', 'd1_mbp_noninvasive_min', \n",
    "                'd1_sysbp_noninvasive_max', 'd1_sysbp_noninvasive_min', 'h1_diasbp_noninvasive_max', 'h1_diasbp_noninvasive_min',\n",
    "                'h1_mbp_noninvasive_max', 'h1_mbp_noninvasive_min', 'h1_sysbp_noninvasive_max', 'h1_sysbp_noninvasive_min']\n",
    "to_drop = to_drop + noninvasivelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examined if the columns `patient_id` and `encounter_id` provided any information, or simply serve as a row IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91713"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('patient_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91713"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('encounter_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91713"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of distinct `patient_id` and `encounter_id`'s is equal to the number of rows in the dataset, we drop these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = to_drop + ['encounter_id', 'patient_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([col for col in df.columns if col not in to_drop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a set of columns that contain the minimum and maximum values for certain levels drawn during the first hour or day in the hospital are converted into a single column containing the average of the two values and the original columns are removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_vitals (df, maxvalcol, minvalcol):\n",
    "    rangeval = df.withColumn(colName = 'avgmaxto'+minvalcol, col = (F.col(maxvalcol)+F.col(minvalcol))/2)\n",
    "    newrange = rangeval.drop(maxvalcol, minvalcol)\n",
    "    return newrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgcol = [('d1_diasbp_max', 'd1_diasbp_min'), ('d1_heartrate_max', 'd1_heartrate_min'), ('d1_mbp_max', 'd1_mbp_min'), \n",
    "            ('d1_resprate_max', 'd1_resprate_min'), ('d1_spo2_max', 'd1_spo2_min'), ('d1_sysbp_max', 'd1_sysbp_min'), \n",
    "            ('d1_temp_max', 'd1_temp_min'), ('h1_diasbp_max', 'h1_diasbp_min'), ('h1_heartrate_max', 'h1_heartrate_min'), \n",
    "            ('h1_mbp_max', 'h1_mbp_min'), ('h1_resprate_max', 'h1_resprate_min'), ('h1_spo2_max', 'h1_spo2_min'), \n",
    "            ('h1_sysbp_max', 'h1_sysbp_min'), ('h1_temp_max', 'h1_temp_min'), ('d1_bun_max', 'd1_bun_min'), ('d1_calcium_max', \n",
    "            'd1_calcium_min'), ('d1_creatinine_max', 'd1_creatinine_min'), ('d1_glucose_max', 'd1_glucose_min'), \n",
    "            ('d1_hco3_max', 'd1_hco3_min'), ('d1_hemaglobin_max', 'd1_hemaglobin_min'), ('d1_hematocrit_max', 'd1_hematocrit_min'), \n",
    "            ('d1_platelets_max', 'd1_platelets_min'), ('d1_potassium_max', 'd1_potassium_min'), ('d1_sodium_max', 'd1_sodium_min'), \n",
    "            ('d1_wbc_max', 'd1_wbc_min')]\n",
    "\n",
    "for colnam in avgcol:\n",
    "    avgdf = range_vitals(df, colnam[0], colnam[1])\n",
    "    df = avgdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all missing values in the string columns are converted to `None` type so that they can be later be imputed or dropped as necessary with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace='NA',\n",
    "                value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"/project/ds5559/fa21-group04/data/df.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
