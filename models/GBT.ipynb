{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 | Model Building | Gradient Boosted Trees\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session and load the data from the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04GBT\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline, PipelineModel  \n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/project/ds5559/fa21-group04/data/processed_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a training/validation test split on the data, reserving 80% of the data for training and validation and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal, test = df.randomSplit([0.8, 0.2], seed=304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a feature vector is constructed using the selected features from the `Group04FeatureSelection.ipynb` notebook. The columns selected as inputs are features chosen by a `UnivariateFeatureSelector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_vectorizer =  VectorAssembler(inputCols=['FinalCatFeatures',\n",
    "                                                       'selectedContFeatures'],\n",
    "                                            outputCol='features',\n",
    "                                            handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we select the features we want in model manually.from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'hospital_death')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_pipeline = Pipeline(stages=[final_feature_vectorizer,\n",
    "                               gbt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train and tune our model, we create a paramGrid of possible hyperparameter values for the model and pass it to a `CrossValidator` to determine the optimal values for the hyperparameters. For the Gradient Boosted Tree Model we chose `maxBins`, the max number of bins for discretizing continuous features, and `maxDepth`, th maximum depth of the tree, as the tunable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [5, 10]) \\\n",
    "    .addGrid(gbt.maxBins, [10, 20]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose `k=5` as the number of folds for the `CrossValidator` and selected AUC as the metric to determine the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setLabelCol(gbt.getLabelCol()),\n",
    "                          numFolds=5,\n",
    "                          seed=304,\n",
    "                          parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbt_model = crossval.fit(trainVal)\n",
    "gbt_model = CrossValidatorModel.load('/project/ds5559/fa21-group04/models/gbtc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained using the trainVal data. For expediency in notebook editing, the trained model may be loaded from the location shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, the test set is predicted using the chosen model from the `CrossValidator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gbt_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are then passed to two objects:\n",
    "\n",
    "* `BinaryClassificationEvaluator` - for AUC metric\n",
    "* `MulticlassClassificationEvaluator` - for confusion matrix and all other model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator = BinaryClassificationEvaluator(rawPredictionCol='probability',\n",
    "                                              labelCol='hospital_death',\n",
    "                                              metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator2 = MulticlassClassificationEvaluator(labelCol='hospital_death',\n",
    "                                                   predictionCol=\"prediction\",\n",
    "                                                   probabilityCol='probability',\n",
    "                                                   metricLabel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92372634643377"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6817288801571709"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"precisionByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28142741281427414"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"recallByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39839265212399544"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"fMeasureByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7185725871857258"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"truePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012952746461981291"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"falsePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8715263491374267"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator.evaluate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12345.,   162.],\n",
       "       [  886.,   347.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = preds.select(['prediction','hospital_death']).withColumn('label', F.col('hospital_death').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "conf_matrix = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "conf_matrix.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|13231|\n",
      "|       1.0|  509|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the low values of the model metrics and the high AUC, that the imbalanced dataset coupled with Spark's default behavior of selecting the higher porbability result as the reported result, is causing the metrics to suffer. Therefore, a probability cutoff value must be selected to reduce the effects of the imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters selected by the `CrossValidator` are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_e512e9cafe8d', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 20,\n",
       " Param(parent='GBTClassifier_e512e9cafe8d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.getEstimatorParamMaps()[ np.argmax(gbt_model.avgMetrics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen value for `maxBins` is 20 and `maxDepth` is 5. As both of these values were the maximum in the paramGrid, it may be prudent to explore a largae parameter space in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|hospital_death|count|\n",
      "+--------------+-----+\n",
      "|             1| 1233|\n",
      "|             0|12507|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.groupBy('hospital_death').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbt_model.write().overwrite().save(\"/project/ds5559/fa21-group04/models/gbtc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Threshold Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we chose to set a probability cutoff value for determining `hospital_death`. To do this, we separate out the probability of `hospital_death = 1` from raw prediction column and then compare the model accuracy, precision, recall, and F1 metrics for different probability cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rawPrediction: vector, hospital_death: double, probability: float, prediction: double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "getprob = udf(lambda v:float(v[1]),FloatType())\n",
    "## Select out the necessary columns\n",
    "output = preds.select(col(\"rawPrediction\"),\n",
    "                              col(\"hospital_death\").cast(DoubleType()),\n",
    "                              getprob(col(\"probability\")).alias(\"probability\"),\n",
    "                              col(\"prediction\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cutoff =  0.05\n",
      "Accuracy: 0.3516 Recall: 0.9862 Precision:  0.1203 F1 score: 0.2144 TP 1216 FP 8892 FN 17 TN 3615\n",
      "Testing cutoff =  0.10\n",
      "Accuracy: 0.7985 Recall: 0.7802 Precision:  0.2780 F1 score: 0.4100 TP 962 FP 2498 FN 271 TN 10009\n",
      "Testing cutoff =  0.15\n",
      "Accuracy: 0.8687 Recall: 0.6504 Precision:  0.3687 F1 score: 0.4707 TP 802 FP 1373 FN 431 TN 11134\n",
      "Testing cutoff =  0.20\n",
      "Accuracy: 0.8941 Recall: 0.5523 Precision:  0.4299 F1 score: 0.4835 TP 681 FP 903 FN 552 TN 11604\n",
      "Testing cutoff =  0.25\n",
      "Accuracy: 0.9066 Recall: 0.5036 Precision:  0.4807 F1 score: 0.4919 TP 621 FP 671 FN 612 TN 11836\n",
      "Testing cutoff =  0.30\n",
      "Accuracy: 0.9134 Recall: 0.4647 Precision:  0.5195 F1 score: 0.4906 TP 573 FP 530 FN 660 TN 11977\n",
      "Testing cutoff =  0.35\n",
      "Accuracy: 0.9189 Recall: 0.4169 Precision:  0.5655 F1 score: 0.4799 TP 514 FP 395 FN 719 TN 12112\n",
      "Testing cutoff =  0.40\n",
      "Accuracy: 0.9234 Recall: 0.3796 Precision:  0.6190 F1 score: 0.4706 TP 468 FP 288 FN 765 TN 12219\n",
      "Testing cutoff =  0.45\n",
      "Accuracy: 0.9241 Recall: 0.3252 Precision:  0.6552 F1 score: 0.4347 TP 401 FP 211 FN 832 TN 12296\n",
      "Testing cutoff =  0.50\n",
      "Accuracy: 0.9237 Recall: 0.2814 Precision:  0.6817 F1 score: 0.3984 TP 347 FP 162 FN 886 TN 12345\n",
      "Testing cutoff =  0.55\n",
      "Accuracy: 0.9226 Recall: 0.2482 Precision:  0.6923 F1 score: 0.3654 TP 306 FP 136 FN 927 TN 12371\n",
      "Testing cutoff =  0.60\n",
      "Accuracy: 0.9216 Recall: 0.2182 Precision:  0.7042 F1 score: 0.3331 TP 269 FP 113 FN 964 TN 12394\n",
      "Testing cutoff =  0.65\n",
      "Accuracy: 0.9212 Recall: 0.1865 Precision:  0.7419 F1 score: 0.2981 TP 230 FP 80 FN 1003 TN 12427\n",
      "Testing cutoff =  0.70\n",
      "Accuracy: 0.9200 Recall: 0.1468 Precision:  0.7939 F1 score: 0.2478 TP 181 FP 47 FN 1052 TN 12460\n",
      "Testing cutoff =  0.75\n",
      "Accuracy: 0.9179 Recall: 0.1054 Precision:  0.8387 F1 score: 0.1873 TP 130 FP 25 FN 1103 TN 12482\n",
      "Testing cutoff =  0.80\n",
      "Accuracy: 0.9163 Recall: 0.0762 Precision:  0.8952 F1 score: 0.1405 TP 94 FP 11 FN 1139 TN 12496\n",
      "Testing cutoff =  0.85\n",
      "Accuracy: 0.9145 Recall: 0.0495 Precision:  0.9531 F1 score: 0.0941 TP 61 FP 3 FN 1172 TN 12504\n",
      "Testing cutoff =  0.90\n",
      "Accuracy: 0.9117 Recall: 0.0170 Precision:  0.9545 F1 score: 0.0335 TP 21 FP 1 FN 1212 TN 12506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cutoff: string, accuracy: string, recall: string, precision: string, F1: string, TP: bigint, FP: bigint, FN: bigint, TN: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "performance_df = spark.createDataFrame([(0,0,0,0,0,0,0,0,0)], ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "for cutoff in range(5, 95, 5):\n",
    "    cutoff = (cutoff * 0.01)\n",
    "  \n",
    "    print('Testing cutoff = ', str(format(cutoff, '.2f')))\n",
    "    lrpredictions_prob_temp = output.withColumn('prediction', when(col('probability') >= cutoff, 1).otherwise(0).cast(DoubleType()))\n",
    "    tp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    tn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    fp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    fn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    a = ((tp + tn)/lrpredictions_prob_temp.count())\n",
    "    if(tp + fn == 0.0):\n",
    "        r = 0.0\n",
    "        p = float(tp) / (tp + fp)\n",
    "    elif(tp + fp == 0.0):\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = 0.0\n",
    "    else:\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = float(tp) / (tp + fp)\n",
    "    \n",
    "    if(p + r == 0):\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * ((p * r)/(p + r))\n",
    "    print(\"Accuracy:\", format(a, '.4f'), \"Recall:\", format(r, '.4f'), \"Precision: \", format(p, '.4f'), \"F1 score:\", format(f1, '.4f'), \"TP\", tp, \"FP\", fp, \"FN\", fn, \"TN\", tn)\n",
    "    performance_df_row = spark.createDataFrame([(format(cutoff, '.2f'),format(a, '.4f'), format(r, '.4f'), format(p, '.4f'), format(f1, '.4f'), tp, fp, fn, tn)], \n",
    "                                               ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "    performance_df = performance_df.union(performance_df_row)\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|cutoff|accuracy|recall|precision|    F1|  TP|  FP|  FN|   TN|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|     0|       0|     0|        0|     0|   0|   0|   0|    0|\n",
      "|  0.05|  0.3516|0.9862|   0.1203|0.2144|1216|8892|  17| 3615|\n",
      "|  0.10|  0.7985|0.7802|   0.2780|0.4100| 962|2498| 271|10009|\n",
      "|  0.15|  0.8687|0.6504|   0.3687|0.4707| 802|1373| 431|11134|\n",
      "|  0.20|  0.8941|0.5523|   0.4299|0.4835| 681| 903| 552|11604|\n",
      "|  0.25|  0.9066|0.5036|   0.4807|0.4919| 621| 671| 612|11836|\n",
      "|  0.30|  0.9134|0.4647|   0.5195|0.4906| 573| 530| 660|11977|\n",
      "|  0.35|  0.9189|0.4169|   0.5655|0.4799| 514| 395| 719|12112|\n",
      "|  0.40|  0.9234|0.3796|   0.6190|0.4706| 468| 288| 765|12219|\n",
      "|  0.45|  0.9241|0.3252|   0.6552|0.4347| 401| 211| 832|12296|\n",
      "|  0.50|  0.9237|0.2814|   0.6817|0.3984| 347| 162| 886|12345|\n",
      "|  0.55|  0.9226|0.2482|   0.6923|0.3654| 306| 136| 927|12371|\n",
      "|  0.60|  0.9216|0.2182|   0.7042|0.3331| 269| 113| 964|12394|\n",
      "|  0.65|  0.9212|0.1865|   0.7419|0.2981| 230|  80|1003|12427|\n",
      "|  0.70|  0.9200|0.1468|   0.7939|0.2478| 181|  47|1052|12460|\n",
      "|  0.75|  0.9179|0.1054|   0.8387|0.1873| 130|  25|1103|12482|\n",
      "|  0.80|  0.9163|0.0762|   0.8952|0.1405|  94|  11|1139|12496|\n",
      "|  0.85|  0.9145|0.0495|   0.9531|0.0941|  61|   3|1172|12504|\n",
      "|  0.90|  0.9117|0.0170|   0.9545|0.0335|  21|   1|1212|12506|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output shown above, when evaluating models by F1 score, the optimal cutoff value is 0.25."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
