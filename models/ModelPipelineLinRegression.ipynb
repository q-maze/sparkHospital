{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 Model Building: Logistic Regression\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04model\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce errors and runtime with Spark inference of the schema and without hard-coding the column types we use the supplied data dictionary to set the column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemadf = pd.read_csv('/project/ds5559/fa21-group04/data/WiDS_Datathon_2020_Dictionary.csv')\n",
    "schemadf = schemadf[schemadf['Variable Name'] != 'icu_admit_type'] # added to address error in data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# define a python dictionary to map the data types from the data dictionary to the column types in spark schema.\n",
    "data_types = {\n",
    "    'integer': IntegerType(),\n",
    "    'binary': IntegerType(),\n",
    "    'numeric': FloatType(),\n",
    "    'string': StringType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pyspark schema for the dataframe\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(row['Variable Name'] , \n",
    "                    data_types[row['Data Type']], \n",
    "                    True) for index, row in schemadf.iterrows()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data using the schema\n",
    "df = spark.read.format('csv') \\\n",
    "    .schema(schema) \\\n",
    "    .option('header', True) \\\n",
    "    .load('/project/ds5559/fa21-group04/data/training_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the data manually, it was identified that `bmi` was type Float, not String as the data dictionary indicated, so this change was manually coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"bmi\",df.bmi.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the columns that contained more than 50% missing data were removed from the dataset. Note that the dataframe was selected from its own columns excluding the columns to drop instead of dropping the columns, as this coerced all columns to type string as a byproduct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "null_counts = df.select([((F.count(F.when(F.isnan(c) | F.col(c).isNull() | F.col(c).contains('NA'), c)))*100/df.count()).alias(c)  for c in df.columns]).collect()[0].asDict()\n",
    "to_drop = [k for k, v in null_counts.items() if v > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data dictionary, the columns labeled in Category \"APACHE covariate\" are all factors in the variables `apache_4a_icu_death_prob` and `apache_4a_hospital_death_prob` and therefore we can remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apachelist = ['albumin_apache', 'apache_2_diagnosis', 'apache_3j_diagnosis', 'apache_post_operative', 'arf_apache', \n",
    "                'bilirubin_apache', 'bun_apache', 'creatinine_apache', 'fio2_apache', 'gcs_eyes_apache', 'gcs_motor_apache', \n",
    "                'gcs_unable_apache', 'gcs_verbal_apache', 'glucose_apache', 'heart_rate_apache', 'hematocrit_apache', \n",
    "                'intubated_apache', 'map_apache', 'paco2_apache', 'paco2_for_ph_apache', 'pao2_apache', 'ph_apache', \n",
    "                'resprate_apache', 'sodium_apache', 'temp_apache', 'urineoutput_apache', 'ventilated_apache', 'wbc_apache']\n",
    "to_drop = to_drop + apachelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data dictionary, the columns labeled in Category indicating \"noninvasive\" are removed as they are included in the equivalent measurement which are measured either invasive or non invasive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noninvasivelist = ['d1_diasbp_noninvasive_max', 'd1_diasbp_noninvasive_min', 'd1_mbp_noninvasive_max', 'd1_mbp_noninvasive_min', \n",
    "                'd1_sysbp_noninvasive_max', 'd1_sysbp_noninvasive_min', 'h1_diasbp_noninvasive_max', 'h1_diasbp_noninvasive_min',\n",
    "                'h1_mbp_noninvasive_max', 'h1_mbp_noninvasive_min', 'h1_sysbp_noninvasive_max', 'h1_sysbp_noninvasive_min']\n",
    "to_drop = to_drop + noninvasivelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examined if the columns `patient_id` and `encounter_id` provided any information, or simply serve as a row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91713"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('patient_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91713"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('encounter_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91713"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of distinct `patient_id` and `encounter_id`'s is equal to the number of rows in the dataset, we drop these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = to_drop + ['encounter_id', 'patient_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([col for col in df.columns if col not in to_drop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, all missing values in the string columns are converted to `None` type so that they can be later be imputed or dropped as necessary with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace='NA',\n",
    "                value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is then persisted for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hospital_id: int, hospital_death: int, age: float, bmi: float, elective_surgery: int, ethnicity: string, gender: string, height: float, hospital_admit_source: string, icu_admit_source: string, icu_id: int, icu_stay_type: string, icu_type: string, pre_icu_los_days: float, readmission_status: int, weight: float, d1_diasbp_max: float, d1_diasbp_min: float, d1_heartrate_max: float, d1_heartrate_min: float, d1_mbp_max: float, d1_mbp_min: float, d1_resprate_max: float, d1_resprate_min: float, d1_spo2_max: float, d1_spo2_min: float, d1_sysbp_max: float, d1_sysbp_min: float, d1_temp_max: float, d1_temp_min: float, h1_diasbp_max: float, h1_diasbp_min: float, h1_heartrate_max: float, h1_heartrate_min: float, h1_mbp_max: float, h1_mbp_min: float, h1_resprate_max: float, h1_resprate_min: float, h1_spo2_max: float, h1_spo2_min: float, h1_sysbp_max: float, h1_sysbp_min: float, h1_temp_max: float, h1_temp_min: float, d1_bun_max: float, d1_bun_min: float, d1_calcium_max: float, d1_calcium_min: float, d1_creatinine_max: float, d1_creatinine_min: float, d1_glucose_max: float, d1_glucose_min: float, d1_hco3_max: float, d1_hco3_min: float, d1_hemaglobin_max: float, d1_hemaglobin_min: float, d1_hematocrit_max: float, d1_hematocrit_min: float, d1_platelets_max: float, d1_platelets_min: float, d1_potassium_max: float, d1_potassium_min: float, d1_sodium_max: float, d1_sodium_min: float, d1_wbc_max: float, d1_wbc_min: float, apache_4a_hospital_death_prob: float, apache_4a_icu_death_prob: float, aids: int, cirrhosis: int, diabetes_mellitus: int, hepatic_failure: int, immunosuppression: int, leukemia: int, lymphoma: int, solid_tumor_with_metastasis: int, apache_3j_bodysystem: string, apache_2_bodysystem: string]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the columns to include in the dataset, the data must now be cleaned. Where above we dropped columns with more than 50% missing values, we must now deal with the remaining columns which may still contain NA values.\n",
    "\n",
    "To begin, it is worthwhile to explore the reamining dataframe columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary of variable names to original datatype for processing pipleines and data exploration\n",
    "var_types = dict(zip(schemadf['Variable Name'], schemadf['Data Type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|summary|       hospital_id|               age|            height|            icu_id|  pre_icu_los_days|            weight|    d1_diasbp_max|     d1_diasbp_min|  d1_heartrate_max| d1_heartrate_min|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|  count|             91713|             87485|             90379|             91713|             91713|             88993|            91548|             91548|             91568|            91568|\n",
      "|   mean|105.66926171862222|62.309515917014345|169.64158813704822| 508.3576919302607|0.8357660508174694| 84.02833953940066|88.49187311574256| 50.16131428321755|103.00056788397694|70.32184824392802|\n",
      "| stddev|62.854406431361824|16.775118786639798| 10.79537824215423|228.98966093955715|  2.48775617608604|25.011496835165584|19.79837938109356|13.317586006867662|22.017346071288163|17.11590251653574|\n",
      "|    min|                 2|              16.0|             137.2|                82|        -24.947222|              38.6|             46.0|              13.0|              58.0|              0.0|\n",
      "|    25%|                47|              52.0|             162.5|               369|       0.035416666|              66.8|             75.0|              42.0|              87.0|             60.0|\n",
      "|    50%|               109|              65.0|             170.1|               504|         0.1388889|              80.3|             86.0|              50.0|             101.0|             69.0|\n",
      "|    75%|               161|              75.0|             177.8|               679|        0.40902779|              97.1|             99.0|              58.0|             116.0|             81.0|\n",
      "|    max|               204|              89.0|            195.59|               927|         159.09097|             186.0|            165.0|              90.0|             177.0|            175.0|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chosen_columns = ['summary'] + [col for col in df.columns if var_types[col] in ['numeric', 'integer']][:10] # select only numeric or integer columns and summary col\n",
    "df_summary.select(chosen_columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that some of the variables, including age and height, may be candidates for imputing with the mean or median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the features that we would like to include in our model and parse them into lists for each datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['age','elective_surgery', 'icu_admit_source', 'gender', 'weight'] # select the features we want in model manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_vars = [] # any variables that we want to give special treatment (bucketize, etc)\n",
    "model_int_vars = [col for col in model_features if var_types[col] == 'integer' and col not in special_vars]\n",
    "model_float_vars = [col for col in model_features if var_types[col] == 'numeric' and col not in special_vars]\n",
    "model_binary_vars = [col for col in model_features if var_types[col] == 'binary' and col not in special_vars]\n",
    "model_string_vars = [col for col in model_features if var_types[col] == 'string' and col not in special_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['age', 'weight']\n",
      "['elective_surgery']\n",
      "['icu_admit_source', 'gender']\n"
     ]
    }
   ],
   "source": [
    "print(model_int_vars)\n",
    "print(model_float_vars)\n",
    "print(model_binary_vars)\n",
    "print(model_string_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal, test = df.randomSplit([0.8, 0.2], seed=304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer Variable Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed because only remaining integer variables are hospital_id and hospital_death, which contain no NANs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float Variable Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float variables are passed to a Max Absolute Scaler, which will scale the values but does not impact the spread of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_float_vars = ['{}_imputed'.format(var) for var in model_float_vars]\n",
    "float_imputer = Imputer(inputCols=model_float_vars, outputCols=imputed_float_vars)\n",
    "\n",
    "float_var_vectorizer = VectorAssembler(inputCols=float_imputer.getOutputCols(), outputCol='floatFeatures', handleInvalid='skip')\n",
    "\n",
    "float_MaxAbs_Scaler = MaxAbsScaler(inputCol=float_var_vectorizer.getOutputCol(),\n",
    "                                   outputCol='scaledFloatFeatures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Variable Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary variables are passed to a vectorizer for use in the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_var_vectorizer = VectorAssembler(inputCols=model_binary_vars, \n",
    "                                        outputCol='binaryFeatures',\n",
    "                                        handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Variable Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String variables are all one hot encoded by first passing them to a string indexer and then to a one hot encoder. Finally, the one hot encoded variables are passed to a vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_string_vars = ['{}_indexed'.format(var) for var in model_string_vars]\n",
    "string_var_indexer = StringIndexer(inputCols=model_string_vars,\n",
    "                                   outputCols=indexed_string_vars,\n",
    "                                   handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_string_vars = ['{}_ohe'.format(var) for var in model_string_vars]\n",
    "string_var_ohe = OneHotEncoder(inputCols=string_var_indexer.getOutputCols(),\n",
    "                               outputCols=ohe_string_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_var_vectorizer = VectorAssembler(inputCols=string_var_ohe.getOutputCols(),\n",
    "                                        outputCol='oheStringFeatures',\n",
    "                                        handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Pipeline Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the pipeline, all variable type vectors are passed to a final vectorizer and then the pipeline is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_vectorizer = VectorAssembler(inputCols=[binary_var_vectorizer.getOutputCol(),\n",
    "                                                     float_MaxAbs_Scaler.getOutputCol(),\n",
    "                                                     string_var_vectorizer.getOutputCol()],\n",
    "                                          outputCol='features', handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol='hospital_death', featuresCol='features', maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline(stages=[float_imputer,\n",
    "                               float_var_vectorizer,\n",
    "                               binary_var_vectorizer,\n",
    "                               float_MaxAbs_Scaler,\n",
    "                               string_var_indexer,\n",
    "                               string_var_ohe,\n",
    "                               string_var_vectorizer,\n",
    "                               final_feature_vectorizer,\n",
    "                               lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model K-Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to set metricName\n",
    "# model_evaluator = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setLabelCol(lr.getLabelCol()),\n",
    "                          numFolds=5,\n",
    "                          seed=304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(trainVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator = BinaryClassificationEvaluator(rawPredictionCol='probability',\n",
    "                                              labelCol='hospital_death',\n",
    "                                              metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675595877508442"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator.evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainValPreds = cvModel.transform(trainVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6656484892015455"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator.evaluate(trainValPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|73203|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainValPreds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|18373|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|hospital_death|count|\n",
      "+--------------+-----+\n",
      "|             1| 7915|\n",
      "|             0|83798|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('hospital_death').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
