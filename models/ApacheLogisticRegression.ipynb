{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 | Model Building | APACHE Variable Model -Logistic Regression\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session and load the data from the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04Base\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline, PipelineModel  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/project/ds5559/fa21-group04/data/processed_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"APACHE model\", we included variables containing APACHE hospital and icu death probabilities. The data dictionary explains these two variables as probabilistic predictions of mortality for a patient utilizing the APACHE III score and other covariates, including diagnosis. This logistic regression model was explored to determine if the `hospital_death` variable could be determined only using the two apache variables within this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(apache_4a_hospital_death_prob_imputed=0.10000000149011612, apache_4a_icu_death_prob_imputed=0.05000000074505806, hospital_death=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_to_keep = [\"apache_4a_hospital_death_prob_imputed\", \n",
    "              \"apache_4a_icu_death_prob_imputed\",\n",
    "               \"hospital_death\"]\n",
    "apache_df = df[vars_to_keep]\n",
    "apache_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a training/validation test split on the data, reserving 80% of the data for training and validation and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal, test = apache_df.randomSplit([0.8, 0.2], seed=304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vectorizer =  VectorAssembler(inputCols=[\"apache_4a_hospital_death_prob_imputed\", \n",
    "                                    \"apache_4a_icu_death_prob_imputed\"],\n",
    "                                    outputCol='features',\n",
    "                                    handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_lr = LogisticRegression(labelCol='hospital_death', featuresCol='features', maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_lr_pipeline = Pipeline(stages=[final_vectorizer,\n",
    "                               apache_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(apache_lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=apache_lr_pipeline,\n",
    "                          estimatorParamMaps=apache_lr_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setLabelCol(apache_lr.getLabelCol()),\n",
    "                          numFolds=5,\n",
    "                          seed=304,\n",
    "                          parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_model = crossval.fit(trainVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = apache_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator = BinaryClassificationEvaluator(rawPredictionCol='probability',\n",
    "                                              labelCol='hospital_death',\n",
    "                                              metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator2 = MulticlassClassificationEvaluator(labelCol='hospital_death',\n",
    "                                                   predictionCol=\"prediction\",\n",
    "                                                   probabilityCol='probability',\n",
    "                                                    metricLabel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235112936344969"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6867088607594937"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"precisionByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18690783807062877"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"recallByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29383886255924174"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"fMeasureByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18690783807062877"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"truePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007935871743486974"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"falsePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835780984881046"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator.evaluate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12376.,    99.],\n",
       "       [  944.,   217.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = preds.select(['prediction','hospital_death']).withColumn('label', F.col('hospital_death').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "conf_matrix = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "conf_matrix.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|13320|\n",
      "|       1.0|  316|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_d16c95465cea', name='regParam', doc='regularization parameter (>= 0).'): 0.01}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_model.getEstimatorParamMaps()[ np.argmax(apache_model.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutoff Value Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the cutoff value is 0.5, however, we wanted to determine a cutoff value that resulted in the highest F1 value. F1 is the weighted average of precision and recall, which we believe is important for this scenario. We decided to not use accuracy as the threshold determination because F1 is usually more useful than accuracy, especially with the uneven class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rawPrediction: vector, hospital_death: double, probability: float, prediction: double]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "getprob = udf(lambda v:float(v[1]),FloatType())\n",
    "## Select out the necessary columns\n",
    "output = preds.select(col(\"rawPrediction\"),\n",
    "                              col(\"hospital_death\").cast(DoubleType()),\n",
    "                              getprob(col(\"probability\")).alias(\"probability\"),\n",
    "                              col(\"prediction\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cutoff =  0.05\n",
      "Accuracy: 0.4492 Recall: 0.9388 Precision:  0.1278 F1 score: 0.2250 TP 1090 FP 7440 FN 71 TN 5035\n",
      "Testing cutoff =  0.10\n",
      "Accuracy: 0.8719 Recall: 0.5814 Precision:  0.3487 F1 score: 0.4359 TP 675 FP 1261 FN 486 TN 11214\n",
      "Testing cutoff =  0.15\n",
      "Accuracy: 0.9031 Recall: 0.4625 Precision:  0.4348 F1 score: 0.4482 TP 537 FP 698 FN 624 TN 11777\n",
      "Testing cutoff =  0.20\n",
      "Accuracy: 0.9134 Recall: 0.3902 Precision:  0.4892 F1 score: 0.4341 TP 453 FP 473 FN 708 TN 12002\n",
      "Testing cutoff =  0.25\n",
      "Accuracy: 0.9190 Recall: 0.3471 Precision:  0.5373 F1 score: 0.4218 TP 403 FP 347 FN 758 TN 12128\n",
      "Testing cutoff =  0.30\n",
      "Accuracy: 0.9231 Recall: 0.3127 Precision:  0.5922 F1 score: 0.4092 TP 363 FP 250 FN 798 TN 12225\n",
      "Testing cutoff =  0.35\n",
      "Accuracy: 0.9245 Recall: 0.2782 Precision:  0.6272 F1 score: 0.3854 TP 323 FP 192 FN 838 TN 12283\n",
      "Testing cutoff =  0.40\n",
      "Accuracy: 0.9242 Recall: 0.2403 Precision:  0.6473 F1 score: 0.3505 TP 279 FP 152 FN 882 TN 12323\n",
      "Testing cutoff =  0.45\n",
      "Accuracy: 0.9240 Recall: 0.2093 Precision:  0.6713 F1 score: 0.3191 TP 243 FP 119 FN 918 TN 12356\n",
      "Testing cutoff =  0.50\n",
      "Accuracy: 0.9235 Recall: 0.1869 Precision:  0.6867 F1 score: 0.2938 TP 217 FP 99 FN 944 TN 12376\n",
      "Testing cutoff =  0.55\n",
      "Accuracy: 0.9229 Recall: 0.1585 Precision:  0.7104 F1 score: 0.2592 TP 184 FP 75 FN 977 TN 12400\n",
      "Testing cutoff =  0.60\n",
      "Accuracy: 0.9226 Recall: 0.1395 Precision:  0.7397 F1 score: 0.2348 TP 162 FP 57 FN 999 TN 12418\n",
      "Testing cutoff =  0.65\n",
      "Accuracy: 0.9218 Recall: 0.1154 Precision:  0.7701 F1 score: 0.2007 TP 134 FP 40 FN 1027 TN 12435\n",
      "Testing cutoff =  0.70\n",
      "Accuracy: 0.9202 Recall: 0.0879 Precision:  0.7786 F1 score: 0.1579 TP 102 FP 29 FN 1059 TN 12446\n",
      "Testing cutoff =  0.75\n",
      "Accuracy: 0.9193 Recall: 0.0637 Precision:  0.8506 F1 score: 0.1186 TP 74 FP 13 FN 1087 TN 12462\n",
      "Testing cutoff =  0.80\n",
      "Accuracy: 0.9174 Recall: 0.0336 Precision:  0.9070 F1 score: 0.0648 TP 39 FP 4 FN 1122 TN 12471\n",
      "Testing cutoff =  0.85\n",
      "Accuracy: 0.9152 Recall: 0.0060 Precision:  0.7778 F1 score: 0.0120 TP 7 FP 2 FN 1154 TN 12473\n",
      "Testing cutoff =  0.90\n",
      "Accuracy: 0.9149 Recall: 0.0000 Precision:  0.0000 F1 score: 0.0000 TP 0 FP 0 FN 1161 TN 12475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cutoff: string, accuracy: string, recall: string, precision: string, F1: string, TP: bigint, FP: bigint, FN: bigint, TN: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "performance_df = spark.createDataFrame([(0,0,0,0,0,0,0,0,0)], ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "for cutoff in range(5, 95, 5):\n",
    "    cutoff = (cutoff * 0.01)\n",
    "  \n",
    "    print('Testing cutoff = ', str(format(cutoff, '.2f')))\n",
    "    lrpredictions_prob_temp = output.withColumn('prediction', when(col('probability') >= cutoff, 1).otherwise(0).cast(DoubleType()))\n",
    "    tp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    tn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    fp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    fn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    a = ((tp + tn)/lrpredictions_prob_temp.count())\n",
    "    if(tp + fn == 0.0):\n",
    "        r = 0.0\n",
    "        p = float(tp) / (tp + fp)\n",
    "    elif(tp + fp == 0.0):\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = 0.0\n",
    "    else:\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = float(tp) / (tp + fp)\n",
    "    \n",
    "    if(p + r == 0):\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * ((p * r)/(p + r))\n",
    "    print(\"Accuracy:\", format(a, '.4f'), \"Recall:\", format(r, '.4f'), \"Precision: \", format(p, '.4f'), \"F1 score:\", format(f1, '.4f'), \"TP\", tp, \"FP\", fp, \"FN\", fn, \"TN\", tn)\n",
    "    performance_df_row = spark.createDataFrame([(format(cutoff, '.2f'),format(a, '.4f'), format(r, '.4f'), format(p, '.4f'), format(f1, '.4f'), tp, fp, fn, tn)], \n",
    "                                               ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "    performance_df = performance_df.union(performance_df_row)\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|cutoff|accuracy|recall|precision|    F1|  TP|  FP|  FN|   TN|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|     0|       0|     0|        0|     0|   0|   0|   0|    0|\n",
      "|  0.05|  0.4492|0.9388|   0.1278|0.2250|1090|7440|  71| 5035|\n",
      "|  0.10|  0.8719|0.5814|   0.3487|0.4359| 675|1261| 486|11214|\n",
      "|  0.15|  0.9031|0.4625|   0.4348|0.4482| 537| 698| 624|11777|\n",
      "|  0.20|  0.9134|0.3902|   0.4892|0.4341| 453| 473| 708|12002|\n",
      "|  0.25|  0.9190|0.3471|   0.5373|0.4218| 403| 347| 758|12128|\n",
      "|  0.30|  0.9231|0.3127|   0.5922|0.4092| 363| 250| 798|12225|\n",
      "|  0.35|  0.9245|0.2782|   0.6272|0.3854| 323| 192| 838|12283|\n",
      "|  0.40|  0.9242|0.2403|   0.6473|0.3505| 279| 152| 882|12323|\n",
      "|  0.45|  0.9240|0.2093|   0.6713|0.3191| 243| 119| 918|12356|\n",
      "|  0.50|  0.9235|0.1869|   0.6867|0.2938| 217|  99| 944|12376|\n",
      "|  0.55|  0.9229|0.1585|   0.7104|0.2592| 184|  75| 977|12400|\n",
      "|  0.60|  0.9226|0.1395|   0.7397|0.2348| 162|  57| 999|12418|\n",
      "|  0.65|  0.9218|0.1154|   0.7701|0.2007| 134|  40|1027|12435|\n",
      "|  0.70|  0.9202|0.0879|   0.7786|0.1579| 102|  29|1059|12446|\n",
      "|  0.75|  0.9193|0.0637|   0.8506|0.1186|  74|  13|1087|12462|\n",
      "|  0.80|  0.9174|0.0336|   0.9070|0.0648|  39|   4|1122|12471|\n",
      "|  0.85|  0.9152|0.0060|   0.7778|0.0120|   7|   2|1154|12473|\n",
      "|  0.90|  0.9149|0.0000|   0.0000|0.0000|   0|   0|1161|12475|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
