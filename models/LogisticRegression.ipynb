{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 | Model Building | Logistic Regression\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session and load the data from the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04LogisticNDS\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline, PipelineModel  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/project/ds5559/fa21-group04/data/processed_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a training/validation test split on the data, reserving 80% of the data for training and validation and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal, test = df.randomSplit([0.8, 0.2], seed=304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a feature vector is constructed using the selected features from the `Group04FeatureSelection.ipynb` notebook. The columns selected as inputs are features chosen by a `UnivariateFeatureSelector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_vectorizer =  VectorAssembler(inputCols=['FinalCatFeatures',\n",
    "                                                       'selectedContFeatures'],\n",
    "                                            outputCol='features',\n",
    "                                            handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a `LogisticRegression` classifier object is constructed to predict `hospital_death` based on the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol='hospital_death', featuresCol='features', maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline(stages=[final_feature_vectorizer,\n",
    "                               lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setLabelCol(lr.getLabelCol()),\n",
    "                          numFolds=5,\n",
    "                          seed=304,\n",
    "                          parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg_model = crossval.fit(trainVal)\n",
    "log_reg_model = CrossValidatorModel.load('/project/ds5559/fa21-group04/models/logregmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, the test set is predicted using the chosen model from the `CrossValidator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = log_reg_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are then passed to two objects:\n",
    "\n",
    "* `BinaryClassificationEvaluator` - for AUC metric\n",
    "* `MulticlassClassificationEvaluator` - for confusion matrix and all other model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator = BinaryClassificationEvaluator(rawPredictionCol='probability',\n",
    "                                              labelCol='hospital_death',\n",
    "                                              metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator2 = MulticlassClassificationEvaluator(labelCol='hospital_death',\n",
    "                                                   predictionCol=\"prediction\",\n",
    "                                                   probabilityCol='probability',\n",
    "                                                   metricLabel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178311499272198"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7549019607843137"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"precisionByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12489862124898621"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"recallByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21433542101600556"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"fMeasureByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12489862124898621"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"truePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003997761253697929"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator2.evaluate(preds, {testEvaluator2.metricName: \"falsePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8384881108914761"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator.evaluate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12457.,    50.],\n",
       "       [ 1079.,   154.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = preds.select(['prediction','hospital_death']).withColumn('label', F.col('hospital_death').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "conf_matrix = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "conf_matrix.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|13536|\n",
      "|       1.0|  204|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the low values of the model metrics and the high AUC, that the imbalanced dataset coupled with Spark's default behavior of selecting the higher porbability result as the reported result, is causing the metrics to suffer. Therefore, a probability cutoff value must be selected to reduce the effects of the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg_model.write().overwrite().save(\"/project/ds5559/fa21-group04/models/logregmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters selected by the `CrossValidator` are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_0671f424ae0e', name='regParam', doc='regularization parameter (>= 0).'): 0.1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_model.getEstimatorParamMaps()[ np.argmax(log_reg_model.avgMetrics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen value for `regParam` is 0.1, indicating that there is some benefit in applying a penalty term to the logisitic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Threshold Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we chose to set a probability cutoff value for determining `hospital_death`. To do this, we separate out the probability of `hospital_death = 1` from raw prediction column and then compare the model accuracy, precision, recall, and F1 metrics for different probability cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rawPrediction: vector, hospital_death: double, probability: float, prediction: double]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "getprob = udf(lambda v:float(v[1]),FloatType())\n",
    "## Select out the necessary columns\n",
    "output = preds.select(col(\"rawPrediction\"),\n",
    "                              col(\"hospital_death\").cast(DoubleType()),\n",
    "                              getprob(col(\"probability\")).alias(\"probability\"),\n",
    "                              col(\"prediction\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cutoff =  0.05\n",
      "Accuracy: 0.5684 Recall: 0.8978 Precision:  0.1602 F1 score: 0.2719 TP 1107 FP 5804 FN 126 TN 6703\n",
      "Testing cutoff =  0.10\n",
      "Accuracy: 0.8147 Recall: 0.6951 Precision:  0.2831 F1 score: 0.4023 TP 857 FP 2170 FN 376 TN 10337\n",
      "Testing cutoff =  0.15\n",
      "Accuracy: 0.8825 Recall: 0.5483 Precision:  0.3899 F1 score: 0.4557 TP 676 FP 1058 FN 557 TN 11449\n",
      "Testing cutoff =  0.20\n",
      "Accuracy: 0.9072 Recall: 0.4347 Precision:  0.4811 F1 score: 0.4568 TP 536 FP 578 FN 697 TN 11929\n",
      "Testing cutoff =  0.25\n",
      "Accuracy: 0.9162 Recall: 0.3569 Precision:  0.5507 F1 score: 0.4331 TP 440 FP 359 FN 793 TN 12148\n",
      "Testing cutoff =  0.30\n",
      "Accuracy: 0.9189 Recall: 0.2968 Precision:  0.5961 F1 score: 0.3963 TP 366 FP 248 FN 867 TN 12259\n",
      "Testing cutoff =  0.35\n",
      "Accuracy: 0.9205 Recall: 0.2393 Precision:  0.6570 F1 score: 0.3508 TP 295 FP 154 FN 938 TN 12353\n",
      "Testing cutoff =  0.40\n",
      "Accuracy: 0.9197 Recall: 0.1955 Precision:  0.6847 F1 score: 0.3041 TP 241 FP 111 FN 992 TN 12396\n",
      "Testing cutoff =  0.45\n",
      "Accuracy: 0.9195 Recall: 0.1590 Precision:  0.7396 F1 score: 0.2617 TP 196 FP 69 FN 1037 TN 12438\n",
      "Testing cutoff =  0.50\n",
      "Accuracy: 0.9178 Recall: 0.1249 Precision:  0.7549 F1 score: 0.2143 TP 154 FP 50 FN 1079 TN 12457\n",
      "Testing cutoff =  0.55\n",
      "Accuracy: 0.9167 Recall: 0.0973 Precision:  0.7947 F1 score: 0.1734 TP 120 FP 31 FN 1113 TN 12476\n",
      "Testing cutoff =  0.60\n",
      "Accuracy: 0.9148 Recall: 0.0673 Precision:  0.7981 F1 score: 0.1242 TP 83 FP 21 FN 1150 TN 12486\n",
      "Testing cutoff =  0.65\n",
      "Accuracy: 0.9142 Recall: 0.0552 Precision:  0.8293 F1 score: 0.1034 TP 68 FP 14 FN 1165 TN 12493\n",
      "Testing cutoff =  0.70\n",
      "Accuracy: 0.9127 Recall: 0.0357 Precision:  0.8000 F1 score: 0.0683 TP 44 FP 11 FN 1189 TN 12496\n",
      "Testing cutoff =  0.75\n",
      "Accuracy: 0.9121 Recall: 0.0235 Precision:  0.8788 F1 score: 0.0458 TP 29 FP 4 FN 1204 TN 12503\n",
      "Testing cutoff =  0.80\n",
      "Accuracy: 0.9113 Recall: 0.0122 Precision:  0.9375 F1 score: 0.0240 TP 15 FP 1 FN 1218 TN 12506\n",
      "Testing cutoff =  0.85\n",
      "Accuracy: 0.9105 Recall: 0.0032 Precision:  0.8000 F1 score: 0.0065 TP 4 FP 1 FN 1229 TN 12506\n",
      "Testing cutoff =  0.90\n",
      "Accuracy: 0.9103 Recall: 0.0008 Precision:  1.0000 F1 score: 0.0016 TP 1 FP 0 FN 1232 TN 12507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cutoff: string, accuracy: string, recall: string, precision: string, F1: string, TP: bigint, FP: bigint, FN: bigint, TN: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "performance_df = spark.createDataFrame([(0,0,0,0,0,0,0,0,0)], ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "for cutoff in range(5, 95, 5):\n",
    "    cutoff = (cutoff * 0.01)\n",
    "  \n",
    "    print('Testing cutoff = ', str(format(cutoff, '.2f')))\n",
    "    lrpredictions_prob_temp = output.withColumn('prediction', when(col('probability') >= cutoff, 1).otherwise(0).cast(DoubleType()))\n",
    "    tp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    tn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    fp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    fn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    a = ((tp + tn)/lrpredictions_prob_temp.count())\n",
    "    if(tp + fn == 0.0):\n",
    "        r = 0.0\n",
    "        p = float(tp) / (tp + fp)\n",
    "    elif(tp + fp == 0.0):\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = 0.0\n",
    "    else:\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = float(tp) / (tp + fp)\n",
    "    \n",
    "    if(p + r == 0):\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * ((p * r)/(p + r))\n",
    "    print(\"Accuracy:\", format(a, '.4f'), \"Recall:\", format(r, '.4f'), \"Precision: \", format(p, '.4f'), \"F1 score:\", format(f1, '.4f'), \"TP\", tp, \"FP\", fp, \"FN\", fn, \"TN\", tn)\n",
    "    performance_df_row = spark.createDataFrame([(format(cutoff, '.2f'),format(a, '.4f'), format(r, '.4f'), format(p, '.4f'), format(f1, '.4f'), tp, fp, fn, tn)], \n",
    "                                               ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "    performance_df = performance_df.union(performance_df_row)\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|cutoff|accuracy|recall|precision|    F1|  TP|  FP|  FN|   TN|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|     0|       0|     0|        0|     0|   0|   0|   0|    0|\n",
      "|  0.05|  0.5684|0.8978|   0.1602|0.2719|1107|5804| 126| 6703|\n",
      "|  0.10|  0.8147|0.6951|   0.2831|0.4023| 857|2170| 376|10337|\n",
      "|  0.15|  0.8825|0.5483|   0.3899|0.4557| 676|1058| 557|11449|\n",
      "|  0.20|  0.9072|0.4347|   0.4811|0.4568| 536| 578| 697|11929|\n",
      "|  0.25|  0.9162|0.3569|   0.5507|0.4331| 440| 359| 793|12148|\n",
      "|  0.30|  0.9189|0.2968|   0.5961|0.3963| 366| 248| 867|12259|\n",
      "|  0.35|  0.9205|0.2393|   0.6570|0.3508| 295| 154| 938|12353|\n",
      "|  0.40|  0.9197|0.1955|   0.6847|0.3041| 241| 111| 992|12396|\n",
      "|  0.45|  0.9195|0.1590|   0.7396|0.2617| 196|  69|1037|12438|\n",
      "|  0.50|  0.9178|0.1249|   0.7549|0.2143| 154|  50|1079|12457|\n",
      "|  0.55|  0.9167|0.0973|   0.7947|0.1734| 120|  31|1113|12476|\n",
      "|  0.60|  0.9148|0.0673|   0.7981|0.1242|  83|  21|1150|12486|\n",
      "|  0.65|  0.9142|0.0552|   0.8293|0.1034|  68|  14|1165|12493|\n",
      "|  0.70|  0.9127|0.0357|   0.8000|0.0683|  44|  11|1189|12496|\n",
      "|  0.75|  0.9121|0.0235|   0.8788|0.0458|  29|   4|1204|12503|\n",
      "|  0.80|  0.9113|0.0122|   0.9375|0.0240|  15|   1|1218|12506|\n",
      "|  0.85|  0.9105|0.0032|   0.8000|0.0065|   4|   1|1229|12506|\n",
      "|  0.90|  0.9103|0.0008|   1.0000|0.0016|   1|   0|1232|12507|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regression model, the optimal cutoff value based on F1 score is 0.2. When compared to the downsampled logistic regression model, this value is much lower (0.7 v 0.2). This difference can be explained by the large imbalance in the response variable in our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
