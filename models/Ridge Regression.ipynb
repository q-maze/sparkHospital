{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 04 | Model Building | Ridge Regression\n",
    "\n",
    "-------------------------\n",
    "Amber Curran (akc6be)\n",
    "\n",
    "Manpreet Dhindsa (mkd8bb)\n",
    "\n",
    "Quinton Mays (rub9ez)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To begin we create our Spark Session and load the data from the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"group04RF\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline, PipelineModel  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/project/ds5559/fa21-group04/data/processed_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a training/validation test split on the data, reserving 80% of the data for training and validation and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainValridge, testridge = df.randomSplit([0.8, 0.2], seed=304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a feature vector is constructed using the selected features from the `Group04FeatureSelection.ipynb` notebook. The columns selected as inputs are features chosen by a `UnivariateFeatureSelector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_vectorizer =  VectorAssembler(inputCols=['FinalCatFeatures',\n",
    "                                                       'selectedContFeatures'],\n",
    "                                            outputCol='features',\n",
    "                                            handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a `RidgeRegressionClassifier` is constructed to predict `hospital_death` based on the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = LogisticRegression(featuresCol = 'features', labelCol = 'hospital_death', elasticNetParam=0.0, maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline is then created to feed the selected features to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipeline = Pipeline(stages=[final_feature_vectorizer,\n",
    "                                  ridge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin model training, the necessary packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our parameter grid for tuning our Ridge Regression model. For this model, our tunable hyperparameter is:\n",
    "\n",
    "* lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(ridge.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalridge = CrossValidator(estimator=ridge_pipeline,\n",
    "                          estimatorParamMaps=ridge_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setLabelCol(ridge.getLabelCol()),\n",
    "                          numFolds=5,\n",
    "                          seed=304,\n",
    "                          parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = crossvalridge.fit(trainValridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, the test set is predicted using the chosen model from the `CrossValidator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsridge = ridge_model.transform(testridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are then passed to two objects:\n",
    "\n",
    "* `BinaryClassificationEvaluator` - for AUC metric\n",
    "* `MulticlassClassificationEvaluator` - for confusion matrix and all other model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_evaluator = BinaryClassificationEvaluator(rawPredictionCol='probability',\n",
    "                                              labelCol='hospital_death',\n",
    "                                              metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEvaluator_ridge2 = MulticlassClassificationEvaluator(labelCol='hospital_death',\n",
    "                                                   predictionCol=\"prediction\",\n",
    "                                                   probabilityCol='probability',\n",
    "                                                   metricLabel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9202112056321502"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_ridge2.evaluate(predsridge, {testEvaluator_ridge2.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7746478873239436"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_ridge2.evaluate(predsridge, {testEvaluator_ridge2.metricName: \"precisionByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13692946058091288"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_ridge2.evaluate(predsridge, {testEvaluator_ridge2.metricName: \"recallByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23272214386459802"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_ridge2.evaluate(predsridge, {testEvaluator_ridge2.metricName: \"fMeasureByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13692946058091288"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_ridge2.evaluate(predsridge, {testEvaluator_ridge2.metricName: \"truePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003861314455795994"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEvaluator_ridge2.evaluate(predsridge, {testEvaluator_ridge2.metricName: \"falsePositiveRateByLabel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.840405211038798"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_evaluator.evaluate(predsridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12383.,    48.],\n",
       "       [ 1040.,   165.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels_ridge = predsridge.select(['prediction','hospital_death']).withColumn('label', F.col('hospital_death').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels_ridge = preds_and_labels_ridge.select(['prediction','label'])\n",
    "conf_matrix_ridge = MulticlassMetrics(preds_and_labels_ridge.rdd.map(tuple))\n",
    "conf_matrix_ridge.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|13423|\n",
      "|       1.0|  213|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predsridge.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_fd98b9740d5e', name='regParam', doc='regularization parameter (>= 0).'): 0.1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.getEstimatorParamMaps()[ np.argmax(ridge_model.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutoff Value Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the cutoff value is 0.5, however, we wanted to determine a cutoff value that resulted in the highest F1 value. F1 is the weighted average of precision and recall, which we believe is important for this scenario. We decided to not use accuracy as the threshold determination because F1 is usually more useful than accuracy, especially with the uneven class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rawPrediction: vector, hospital_death: double, probability: float, prediction: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "getprob = udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "output = predsridge.select(col(\"rawPrediction\"),\n",
    "                              col(\"hospital_death\").cast(DoubleType()),\n",
    "                              getprob(col(\"probability\")).alias(\"probability\"),\n",
    "                              col(\"prediction\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines each of the different metrics for cutoff values between 0.05 and 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cutoff =  0.05\n",
      "Accuracy: 0.5612 Recall: 0.9071 Precision:  0.1569 F1 score: 0.2676 TP 1093 FP 5872 FN 112 TN 6559\n",
      "Testing cutoff =  0.10\n",
      "Accuracy: 0.8165 Recall: 0.6971 Precision:  0.2822 F1 score: 0.4017 TP 840 FP 2137 FN 365 TN 10294\n",
      "Testing cutoff =  0.15\n",
      "Accuracy: 0.8843 Recall: 0.5519 Precision:  0.3905 F1 score: 0.4574 TP 665 FP 1038 FN 540 TN 11393\n",
      "Testing cutoff =  0.20\n",
      "Accuracy: 0.9070 Recall: 0.4307 Precision:  0.4714 F1 score: 0.4501 TP 519 FP 582 FN 686 TN 11849\n",
      "Testing cutoff =  0.25\n",
      "Accuracy: 0.9160 Recall: 0.3527 Precision:  0.5373 F1 score: 0.4259 TP 425 FP 366 FN 780 TN 12065\n",
      "Testing cutoff =  0.30\n",
      "Accuracy: 0.9198 Recall: 0.2888 Precision:  0.5959 F1 score: 0.3890 TP 348 FP 236 FN 857 TN 12195\n",
      "Testing cutoff =  0.35\n",
      "Accuracy: 0.9212 Recall: 0.2373 Precision:  0.6485 F1 score: 0.3475 TP 286 FP 155 FN 919 TN 12276\n",
      "Testing cutoff =  0.40\n",
      "Accuracy: 0.9216 Recall: 0.2008 Precision:  0.6954 F1 score: 0.3117 TP 242 FP 106 FN 963 TN 12325\n",
      "Testing cutoff =  0.45\n",
      "Accuracy: 0.9219 Recall: 0.1718 Precision:  0.7555 F1 score: 0.2799 TP 207 FP 67 FN 998 TN 12364\n",
      "Testing cutoff =  0.50\n",
      "Accuracy: 0.9202 Recall: 0.1369 Precision:  0.7746 F1 score: 0.2327 TP 165 FP 48 FN 1040 TN 12383\n",
      "Testing cutoff =  0.55\n",
      "Accuracy: 0.9185 Recall: 0.1046 Precision:  0.7975 F1 score: 0.1849 TP 126 FP 32 FN 1079 TN 12399\n",
      "Testing cutoff =  0.60\n",
      "Accuracy: 0.9174 Recall: 0.0797 Precision:  0.8421 F1 score: 0.1456 TP 96 FP 18 FN 1109 TN 12413\n",
      "Testing cutoff =  0.65\n",
      "Accuracy: 0.9156 Recall: 0.0573 Precision:  0.8214 F1 score: 0.1071 TP 69 FP 15 FN 1136 TN 12416\n",
      "Testing cutoff =  0.70\n",
      "Accuracy: 0.9144 Recall: 0.0407 Precision:  0.8167 F1 score: 0.0775 TP 49 FP 11 FN 1156 TN 12420\n",
      "Testing cutoff =  0.75\n",
      "Accuracy: 0.9138 Recall: 0.0290 Precision:  0.8537 F1 score: 0.0562 TP 35 FP 6 FN 1170 TN 12425\n",
      "Testing cutoff =  0.80\n",
      "Accuracy: 0.9129 Recall: 0.0166 Precision:  0.8696 F1 score: 0.0326 TP 20 FP 3 FN 1185 TN 12428\n",
      "Testing cutoff =  0.85\n",
      "Accuracy: 0.9122 Recall: 0.0066 Precision:  1.0000 F1 score: 0.0132 TP 8 FP 0 FN 1197 TN 12431\n",
      "Testing cutoff =  0.90\n",
      "Accuracy: 0.9119 Recall: 0.0033 Precision:  1.0000 F1 score: 0.0066 TP 4 FP 0 FN 1201 TN 12431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cutoff: string, accuracy: string, recall: string, precision: string, F1: string, TP: bigint, FP: bigint, FN: bigint, TN: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "performance_df = spark.createDataFrame([(0,0,0,0,0,0,0,0,0)], ['cutoff','accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "for cutoff in range(5, 95, 5):\n",
    "    cutoff = (cutoff * 0.01)\n",
    "  \n",
    "    print('Testing cutoff = ', str(format(cutoff, '.2f')))\n",
    "    lrpredictions_prob_temp = output.withColumn('prediction', when(col('probability') >= cutoff, 1).otherwise(0).cast(DoubleType()))\n",
    "    tp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    tn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    fp = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 0) & (lrpredictions_prob_temp.prediction == 1)].count()\n",
    "    fn = lrpredictions_prob_temp[(lrpredictions_prob_temp.hospital_death == 1) & (lrpredictions_prob_temp.prediction == 0)].count()\n",
    "    a = ((tp + tn)/lrpredictions_prob_temp.count())\n",
    "    if(tp + fn == 0.0):\n",
    "        r = 0.0\n",
    "        p = float(tp) / (tp + fp)\n",
    "    elif(tp + fp == 0.0):\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = 0.0\n",
    "    else:\n",
    "        r = float(tp) / (tp + fn)\n",
    "        p = float(tp) / (tp + fp)\n",
    "    \n",
    "    if(p + r == 0):\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * ((p * r)/(p + r))\n",
    "    print(\"Accuracy:\", format(a, '.4f'), \"Recall:\", format(r, '.4f'), \"Precision: \", format(p, '.4f'), \"F1 score:\", format(f1, '.4f'), \"TP\", tp, \"FP\", fp, \"FN\", fn, \"TN\", tn)\n",
    "    performance_df_row = spark.createDataFrame([(format(cutoff, '.2f'), format(a, '.4f'), format(r, '.4f'), format(p, '.4f'), format(f1, '.4f'), tp, fp, fn, tn)], ['cutoff', 'accuracy', 'recall', 'precision', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "    performance_df = performance_df.union(performance_df_row)\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|cutoff|accuracy|recall|precision|    F1|  TP|  FP|  FN|   TN|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "|     0|       0|     0|        0|     0|   0|   0|   0|    0|\n",
      "|  0.05|  0.5612|0.9071|   0.1569|0.2676|1093|5872| 112| 6559|\n",
      "|  0.10|  0.8165|0.6971|   0.2822|0.4017| 840|2137| 365|10294|\n",
      "|  0.15|  0.8843|0.5519|   0.3905|0.4574| 665|1038| 540|11393|\n",
      "|  0.20|  0.9070|0.4307|   0.4714|0.4501| 519| 582| 686|11849|\n",
      "|  0.25|  0.9160|0.3527|   0.5373|0.4259| 425| 366| 780|12065|\n",
      "|  0.30|  0.9198|0.2888|   0.5959|0.3890| 348| 236| 857|12195|\n",
      "|  0.35|  0.9212|0.2373|   0.6485|0.3475| 286| 155| 919|12276|\n",
      "|  0.40|  0.9216|0.2008|   0.6954|0.3117| 242| 106| 963|12325|\n",
      "|  0.45|  0.9219|0.1718|   0.7555|0.2799| 207|  67| 998|12364|\n",
      "|  0.50|  0.9202|0.1369|   0.7746|0.2327| 165|  48|1040|12383|\n",
      "|  0.55|  0.9185|0.1046|   0.7975|0.1849| 126|  32|1079|12399|\n",
      "|  0.60|  0.9174|0.0797|   0.8421|0.1456|  96|  18|1109|12413|\n",
      "|  0.65|  0.9156|0.0573|   0.8214|0.1071|  69|  15|1136|12416|\n",
      "|  0.70|  0.9144|0.0407|   0.8167|0.0775|  49|  11|1156|12420|\n",
      "|  0.75|  0.9138|0.0290|   0.8537|0.0562|  35|   6|1170|12425|\n",
      "|  0.80|  0.9129|0.0166|   0.8696|0.0326|  20|   3|1185|12428|\n",
      "|  0.85|  0.9122|0.0066|   1.0000|0.0132|   8|   0|1197|12431|\n",
      "|  0.90|  0.9119|0.0033|   1.0000|0.0066|   4|   0|1201|12431|\n",
      "+------+--------+------+---------+------+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
